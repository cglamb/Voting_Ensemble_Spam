{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8bba41",
   "metadata": {},
   "source": [
    "# Lamb - Voting Ensemble in Spam Classification: An Approach for Optimizing Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e3592d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50527a9",
   "metadata": {},
   "source": [
    "### Kaggle Database Link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1706802f",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062cf126",
   "metadata": {},
   "source": [
    "### Other Articles to Consider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b193ee5",
   "metadata": {},
   "source": [
    "http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c67ad",
   "metadata": {},
   "source": [
    "https://www.proquest.com/docview/2798556468?pq-origsite=primo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea62c7",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1db544",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe9f8d",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad0d7b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import opendatasets as od\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.metrics import precision_score, recall_score, roc_curve, confusion_matrix, jaccard_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from keras.layers import SimpleRNN, LSTM, Dense, Dropout, Activation, Flatten\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier, AdaBoostClassifier\n",
    "from xgboost import XGBClassifier, XGBRFClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dcfbe1",
   "metadata": {},
   "source": [
    "### Pickle Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4388beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled variables\n",
    "# with open('session.pkl', 'rb') as f:\n",
    "#     data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf304cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #pickeling file to allow for loading calculation directionly into memory without recomputing\n",
    "# ##as this is a computationally intensive wb this allows for incremental progress to be made without recommputing the whole file\n",
    "# with open('session.pkl', 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#             'cnt' : cnt,\n",
    "#             'ltt' : ltt,\n",
    "#             'lrt' : lrt,\n",
    "#             'gnt' : gnt,\n",
    "#             'bnt' : bnt,\n",
    "#             'mbt' : mbt,\n",
    "#             'srt' : srt,\n",
    "#             'spt' : spt,\n",
    "#             'slt' : slt,\n",
    "#             'rft' : rft,\n",
    "#             'gbt' : gbt,\n",
    "#             'ett' : ett,\n",
    "#             'adt' : adt,\n",
    "#             'xgt' : xgt,\n",
    "#             'xrt' : xrt,\n",
    "#             'cnr' : cnr,\n",
    "#             'ltr' : ltr,\n",
    "#             'lrr' : lrr,\n",
    "#             'gnr' : gnr,\n",
    "#             'bnr' : bnr,\n",
    "#             'mbr' : mbr,\n",
    "#             'srr' : srr,\n",
    "#             'spr' : spr,\n",
    "#             'slr' : slr,\n",
    "#             'rfr' : rfr,\n",
    "#             'gbr' : gbr,\n",
    "#             'etr' : etr,\n",
    "#             'adr' : adr,\n",
    "#             'xgr' : xgr,\n",
    "#             'xrr' : xrr\n",
    "#                 }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd5edb",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f49d253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 5)\n"
     ]
    }
   ],
   "source": [
    "#loading corpus into data frame\n",
    "df = pd.read_csv(\"spam.csv\", encoding = \"ISO-8859-1\", engine = \"python\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e761be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581847bc",
   "metadata": {},
   "source": [
    "### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9844f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   v1          5572 non-null   object\n",
      " 1   v2          5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bd168c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>spam</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "      <td>PO Box 5249</td>\n",
       "      <td>MK17 92H. 450Ppw 16\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\Wen u miss someone</td>\n",
       "      <td>the person is definitely special for u..... B...</td>\n",
       "      <td>why to miss them</td>\n",
       "      <td>just Keep-in-touch\\\" gdeve..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\HEY HEY WERETHE MONKEESPEOPLE SAY WE MONKEYAR...</td>\n",
       "      <td>HOWU DOIN? FOUNDURSELF A JOBYET SAUSAGE?LOVE ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671</th>\n",
       "      <td>spam</td>\n",
       "      <td>SMS. ac sun0819 posts HELLO:\\You seem cool</td>\n",
       "      <td>wanted to say hi. HI!!!\\\" Stop? Send STOP to ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>ham</td>\n",
       "      <td>Height of Confidence: All the Aeronautics prof...</td>\n",
       "      <td>this wont even start........ Datz confidence..\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v1                                                 v2  \\\n",
       "95   spam  Your free ringtone is waiting to be collected....   \n",
       "281   ham                                \\Wen u miss someone   \n",
       "444   ham  \\HEY HEY WERETHE MONKEESPEOPLE SAY WE MONKEYAR...   \n",
       "671  spam         SMS. ac sun0819 posts HELLO:\\You seem cool   \n",
       "710   ham  Height of Confidence: All the Aeronautics prof...   \n",
       "\n",
       "                                            Unnamed: 2             Unnamed: 3  \\\n",
       "95                                         PO Box 5249   MK17 92H. 450Ppw 16\"   \n",
       "281   the person is definitely special for u..... B...       why to miss them   \n",
       "444   HOWU DOIN? FOUNDURSELF A JOBYET SAUSAGE?LOVE ...                    NaN   \n",
       "671   wanted to say hi. HI!!!\\\" Stop? Send STOP to ...                    NaN   \n",
       "710    this wont even start........ Datz confidence..\"                    NaN   \n",
       "\n",
       "                         Unnamed: 4  \n",
       "95                              NaN  \n",
       "281   just Keep-in-touch\\\" gdeve..\"  \n",
       "444                             NaN  \n",
       "671                             NaN  \n",
       "710                             NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at the data in the unnamed columns\n",
    "df[df['Unnamed: 2'].isnull() == False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36af2ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>spam</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "      <td>PO Box 5249</td>\n",
       "      <td>MK17 92H. 450Ppw 16\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\Wen u miss someone</td>\n",
       "      <td>the person is definitely special for u..... B...</td>\n",
       "      <td>why to miss them</td>\n",
       "      <td>just Keep-in-touch\\\" gdeve..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>spam</td>\n",
       "      <td>Your free ringtone is waiting to be collected....</td>\n",
       "      <td>PO Box 5249</td>\n",
       "      <td>MK17 92H. 450Ppw 16\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>ham</td>\n",
       "      <td>Edison has rightly said, \\A fool can ask more ...</td>\n",
       "      <td>GN</td>\n",
       "      <td>GE</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\CAN I PLEASE COME UP NOW IMIN TOWN.DONTMATTER...</td>\n",
       "      <td>JUST REALLYNEED 2DOCD.PLEASE DONTPLEASE DONTIG...</td>\n",
       "      <td>U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\"\"</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2  \\\n",
       "95    spam  Your free ringtone is waiting to be collected....   \n",
       "281    ham                                \\Wen u miss someone   \n",
       "899   spam  Your free ringtone is waiting to be collected....   \n",
       "1038   ham  Edison has rightly said, \\A fool can ask more ...   \n",
       "2170   ham  \\CAN I PLEASE COME UP NOW IMIN TOWN.DONTMATTER...   \n",
       "\n",
       "                                             Unnamed: 2  \\\n",
       "95                                          PO Box 5249   \n",
       "281    the person is definitely special for u..... B...   \n",
       "899                                         PO Box 5249   \n",
       "1038                                                 GN   \n",
       "2170  JUST REALLYNEED 2DOCD.PLEASE DONTPLEASE DONTIG...   \n",
       "\n",
       "                                    Unnamed: 3                      Unnamed: 4  \n",
       "95                        MK17 92H. 450Ppw 16\"                             NaN  \n",
       "281                           why to miss them   just Keep-in-touch\\\" gdeve..\"  \n",
       "899                       MK17 92H. 450Ppw 16\"                             NaN  \n",
       "1038                                        GE                         GNT:-)\"  \n",
       "2170  U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\"\"                             NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Unnamed: 3'].isnull() == False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b13b3ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\Wen u miss someone</td>\n",
       "      <td>the person is definitely special for u..... B...</td>\n",
       "      <td>why to miss them</td>\n",
       "      <td>just Keep-in-touch\\\" gdeve..\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>ham</td>\n",
       "      <td>Edison has rightly said, \\A fool can ask more ...</td>\n",
       "      <td>GN</td>\n",
       "      <td>GE</td>\n",
       "      <td>GNT:-)\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>ham</td>\n",
       "      <td>I just lov this line: \\Hurt me with the truth</td>\n",
       "      <td>I don't mind</td>\n",
       "      <td>i wil tolerat.bcs ur my someone..... But</td>\n",
       "      <td>Never comfort me with a lie\\\" gud ni8 and swe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3525</th>\n",
       "      <td>ham</td>\n",
       "      <td>\\HEY BABE! FAR 2 SPUN-OUT 2 SPK AT DA MO... DE...</td>\n",
       "      <td>HAD A COOL NYTHO</td>\n",
       "      <td>TX 4 FONIN HON</td>\n",
       "      <td>CALL 2MWEN IM BK FRMCLOUD 9! J X\\\"\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>ham</td>\n",
       "      <td>When I was born, GOD said, \\Oh No! Another IDI...</td>\n",
       "      <td>GOD said</td>\n",
       "      <td>\\\"OH No! COMPETITION\\\". Who knew</td>\n",
       "      <td>one day these two will become FREINDS FOREVER!\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       v1                                                 v2  \\\n",
       "281   ham                                \\Wen u miss someone   \n",
       "1038  ham  Edison has rightly said, \\A fool can ask more ...   \n",
       "2255  ham      I just lov this line: \\Hurt me with the truth   \n",
       "3525  ham  \\HEY BABE! FAR 2 SPUN-OUT 2 SPK AT DA MO... DE...   \n",
       "4668  ham  When I was born, GOD said, \\Oh No! Another IDI...   \n",
       "\n",
       "                                             Unnamed: 2  \\\n",
       "281    the person is definitely special for u..... B...   \n",
       "1038                                                 GN   \n",
       "2255                                       I don't mind   \n",
       "3525                                   HAD A COOL NYTHO   \n",
       "4668                                           GOD said   \n",
       "\n",
       "                                    Unnamed: 3  \\\n",
       "281                           why to miss them   \n",
       "1038                                        GE   \n",
       "2255  i wil tolerat.bcs ur my someone..... But   \n",
       "3525                            TX 4 FONIN HON   \n",
       "4668          \\\"OH No! COMPETITION\\\". Who knew   \n",
       "\n",
       "                                             Unnamed: 4  \n",
       "281                       just Keep-in-touch\\\" gdeve..\"  \n",
       "1038                                            GNT:-)\"  \n",
       "2255   Never comfort me with a lie\\\" gud ni8 and swe...  \n",
       "3525                CALL 2MWEN IM BK FRMCLOUD 9! J X\\\"\"  \n",
       "4668    one day these two will become FREINDS FOREVER!\"  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['Unnamed: 4'].isnull() == False].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9e6f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    }
   ],
   "source": [
    "#the unknown columns are sparsely populated and most that are are populated appear to contain irrelevant information \n",
    "#(such as time or address info).  droping these columns\n",
    "to_drop = ['Unnamed: 2',\"Unnamed: 3\",\"Unnamed: 4\"]\n",
    "df = df.drop(columns = to_drop)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b269d0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   label      5572 non-null   object\n",
      " 1   documents  5572 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.2+ KB\n"
     ]
    }
   ],
   "source": [
    "#renamining columns\n",
    "rename_list = {'v1':'label','v2':'documents'}\n",
    "df = df.rename(columns=rename_list)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d6f0995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#neither column has any null values, but lets check to make sure there is non-blank text in the documents\n",
    "df_temp = df['documents'].str.len() - df['documents'].str.count(' ')\n",
    "sum(df_temp == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ea9dce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "#okay so all the documents contain at least some characters.  Lets check that our label is a binary indicator as expected\n",
    "label_list = df.label.unique()\n",
    "print(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d647401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating one hotkey on label\n",
    "label_binary = pd.get_dummies(df.label)\n",
    "label_binary= label_binary.drop(columns='ham')\n",
    "label_binary = label_binary.rename(columns={'spam':'label_binary'})\n",
    "df = pd.concat([df,label_binary],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "106c070f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 3)\n",
      "[0]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "#checking hotkey join and binary hotkey labeling\n",
    "print(df.shape)\n",
    "print(df[df['label']=='ham'].label_binary.unique())\n",
    "print(df[df['label']=='spam'].label_binary.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86519632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aac674c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5169, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping duplicated\n",
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b03b2ad",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "360f108d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       documents  label_binary\n",
      "label                         \n",
      "ham         4516          4516\n",
      "spam         653           653\n"
     ]
    }
   ],
   "source": [
    "#looking at the frequency of ham versus spam\n",
    "label_count = df.groupby('label').count()\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f5d15a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5169\n"
     ]
    }
   ],
   "source": [
    "#lets look at how wordy our documents are - first creating a word count\n",
    "documents = df['documents'].tolist()\n",
    "word_count = [] \n",
    "for i in documents:\n",
    "    word_count.append(len(i.split()))\n",
    "print(len(word_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3264a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>min</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max</td>\n",
       "      <td>171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean</td>\n",
       "      <td>15.340685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>std</td>\n",
       "      <td>11.067417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label       value\n",
       "0   min    1.000000\n",
       "1   max  171.000000\n",
       "2  mean   15.340685\n",
       "3   std   11.067417"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating mean, standard deviations, min, and max\n",
    "min_val = min(word_count)\n",
    "max_val =max(word_count)\n",
    "mean_val = np.mean(word_count)\n",
    "var_val = np.std(word_count)\n",
    "stat_label = pd.Series(('min','max','mean','std'))\n",
    "stats = pd.Series((min_val,max_val,mean_val,var_val))\n",
    "d = {'label':stat_label,'value':stats}\n",
    "df_stat = pd.DataFrame(data=d)\n",
    "df_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adee663d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5169, 4)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#adding the word count into the data frame\n",
    "df['word_count'] = np.array(word_count)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdb06c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>documents</th>\n",
       "      <th>label_binary</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>ham</td>\n",
       "      <td>Yup</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>ham</td>\n",
       "      <td>Thanx...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>ham</td>\n",
       "      <td>Okie...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok..</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>ham</td>\n",
       "      <td>Beerage?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label documents  label_binary  word_count\n",
       "260   ham       Yup             0           1\n",
       "275   ham  Thanx...             0           1\n",
       "283   ham   Okie...             0           1\n",
       "286   ham      Ok..             0           1\n",
       "782   ham  Beerage?             0           1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#looking at a few of these one word documents\n",
    "df[df['word_count'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5099d1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003869220352099052"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what percentage of the documents have only 1 word\n",
    "sum(df['word_count'] == 1)/len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a8edca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 5169)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c5eb1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#look at the most common words - first prep a word list\n",
    "word_list = []\n",
    "for i in range(len(documents)):\n",
    "    word_list.append(documents[i].lower().split())\n",
    "master_word_list = list(itertools.chain(*word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d92ba3ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 2095),\n",
       " ('to', 2055),\n",
       " ('you', 1832),\n",
       " ('a', 1281),\n",
       " ('the', 1223),\n",
       " ('and', 919),\n",
       " ('u', 890),\n",
       " ('in', 785),\n",
       " ('is', 766),\n",
       " ('my', 676),\n",
       " ('for', 653),\n",
       " ('your', 618),\n",
       " ('me', 579),\n",
       " ('of', 552),\n",
       " ('have', 532),\n",
       " ('on', 476),\n",
       " ('call', 468),\n",
       " ('are', 457),\n",
       " ('that', 453),\n",
       " ('it', 440)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now count the words\n",
    "count_words = collections.Counter(master_word_list)\n",
    "count_words.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b207acc",
   "metadata": {},
   "source": [
    "### Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef7ebb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making text lowercase\n",
    "df['documents_clean'] = df['documents'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "971e3e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CGLam\\AppData\\Local\\Temp\\ipykernel_5324\\3020070343.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['documents_clean'] = df['documents_clean'].str.replace(r'https?://\\S+|www\\.\\S+', 'url')\n"
     ]
    }
   ],
   "source": [
    "#replacing URLs with keyword \"URL\"\n",
    "df['documents_clean'] = df['documents_clean'].str.replace(r'https?://\\S+|www\\.\\S+', 'url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f10b498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CGLam\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#loading stop words\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02eeb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stop words\n",
    "df['documents_clean'] = df['documents_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1393629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CGLam\\AppData\\Local\\Temp\\ipykernel_5324\\2423228234.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['documents_clean'] = df['documents_clean'].str.replace(r'[^\\w\\s]+', '')\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation\n",
    "df['documents_clean'] = df['documents_clean'].str.replace(r'[^\\w\\s]+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e785dafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('u', 1001),\n",
       " ('call', 487),\n",
       " ('im', 447),\n",
       " ('2', 443),\n",
       " ('get', 364),\n",
       " ('ur', 316),\n",
       " ('go', 269),\n",
       " ('4', 257),\n",
       " ('ltgt', 254),\n",
       " ('ok', 251),\n",
       " ('free', 243),\n",
       " ('know', 239),\n",
       " ('got', 231),\n",
       " ('like', 231),\n",
       " ('good', 217),\n",
       " ('come', 210),\n",
       " ('ill', 206),\n",
       " ('you', 200),\n",
       " ('time', 199),\n",
       " ('now', 198)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re-reviewing most common words to see if it makes sense to create any custom stop words\n",
    "word_list_2 = []\n",
    "documents_2 = df['documents_clean'].tolist()\n",
    "for i in range(len(documents_2)):\n",
    "    word_list_2.append(documents_2[i].lower().split())\n",
    "master_word_list_2 = list(itertools.chain(*word_list_2))\n",
    "count_words_2 = collections.Counter(master_word_list_2)\n",
    "count_words_2.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "417a8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating custom stop words\n",
    "custom_stopwords = {'u','im','ur','ill','you'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "08a75443",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove custom stop words\n",
    "df['documents_clean'] = df['documents_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (custom_stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "67d04734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-character tokens\n",
    "df['documents_clean'] = df['documents_clean'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9a8b950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying stemming\n",
    "stemmer = PorterStemmer()\n",
    "df['documents_clean'] = df['documents_clean'].apply(lambda x: ' '.join([stemmer.stem(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e22fcfe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>documents</th>\n",
       "      <th>label_binary</th>\n",
       "      <th>word_count</th>\n",
       "      <th>documents_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>ok lar joke wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>dun say earli hor c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                          documents  label_binary  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...             0   \n",
       "1   ham                      Ok lar... Joking wif u oni...             0   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...             1   \n",
       "3   ham  U dun say so early hor... U c already then say...             0   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...             0   \n",
       "\n",
       "   word_count                                    documents_clean  \n",
       "0          20  go jurong point crazi avail bugi n great world...  \n",
       "1           6                                ok lar joke wif oni  \n",
       "2          28  free entri wkli comp win fa cup final tkt may ...  \n",
       "3          11                    dun say earli hor c alreadi say  \n",
       "4          13               nah think goe usf live around though  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acff91f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export preprocessed data to excel for further review \n",
    "#df.to_excel('preprocessed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538b9db",
   "metadata": {},
   "source": [
    "### Tokenize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "47a66005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_tokenizer(x):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer\n",
    "    \n",
    "def encode(x2, tokenizer):\n",
    "    encoded_sentences = tokenizer.texts_to_sequences(x2)\n",
    "    encoded_sentences = tf.keras.preprocessing.sequence.pad_sequences(encoded_sentences, padding='post')\n",
    "    return encoded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "489d1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = define_tokenizer(df['documents_clean'])\n",
    "s_strings = encode(df['documents_clean'],tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cded4e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5169"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking that we have appropriate number of documents\n",
    "len(s_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "78447304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go jurong point crazi avail bugi n great world la e buffet cine got amor wat'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quick look at encoding...text of first clean document\n",
    "df['documents_clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8dfa9579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2, 2952,  271,  540,  568,  954,   43,   66,  325,  955,   88,\n",
       "       2089,  956,   11, 2953,   64,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#encoding of that document\n",
    "s_strings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9903ce08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2952\n",
      "271\n",
      "540\n",
      "568\n",
      "954\n",
      "43\n",
      "66\n",
      "325\n",
      "955\n",
      "88\n",
      "2089\n",
      "956\n",
      "11\n",
      "2953\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "#pulling these words out of the dictionary to make sure we encoded as expected\n",
    "d = tokenizer.word_index\n",
    "print(d['go'])\n",
    "print(d['jurong'])\n",
    "print(d['point'])\n",
    "print(d['crazi'])\n",
    "print(d['avail'])\n",
    "print(d['bugi'])\n",
    "print(d['n'])\n",
    "print(d['great'])\n",
    "print(d['world'])\n",
    "print(d['la'])\n",
    "print(d['e'])\n",
    "print(d['buffet'])\n",
    "print(d['cine'])\n",
    "print(d['got'])\n",
    "print(d['amor'])\n",
    "print(d['wat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5663926",
   "metadata": {},
   "source": [
    "### Keras Cross Validation Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da218fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5169, 81)\n",
      "     0     1    2     3     4     5     6    7     8    9  ...  71  72  73  \\\n",
      "0    2  2952  271   540   568   954    43   66   325  955  ...   0   0   0   \n",
      "1    6   226  569   326  1413     0     0    0     0    0  ...   0   0   0   \n",
      "2    9   413  796   797   129  2090  1087  304  2091  206  ...   0   0   0   \n",
      "3  146    40  263  2092    97    73    40    0     0    0  ...   0   0   0   \n",
      "4  750    22  338   696   176   128   339    0     0    0  ...   0   0   0   \n",
      "\n",
      "   74  75  76  77  78  label  \\\n",
      "0   0   0   0   0   0      0   \n",
      "1   0   0   0   0   0      0   \n",
      "2   0   0   0   0   0      1   \n",
      "3   0   0   0   0   0      0   \n",
      "4   0   0   0   0   0      0   \n",
      "\n",
      "                                                 doc  \n",
      "0  go jurong point crazi avail bugi n great world...  \n",
      "1                                ok lar joke wif oni  \n",
      "2  free entri wkli comp win fa cup final tkt may ...  \n",
      "3                    dun say earli hor c alreadi say  \n",
      "4               nah think goe usf live around though  \n",
      "\n",
      "[5 rows x 81 columns]\n"
     ]
    }
   ],
   "source": [
    "#converting tokenized data to pandas dataframe\n",
    "#we will eventually convert back to array but the pd will make the cross validation setup easier\n",
    "s_pandas = pd.DataFrame(data=s_strings)\n",
    "s_pandas['label'] = df['label_binary'].tolist()\n",
    "s_pandas['doc'] = df['documents_clean'].tolist()\n",
    "print(s_pandas.shape)\n",
    "print(s_pandas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "515f5e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584, 80)\n",
      "(2584, 1)\n"
     ]
    }
   ],
   "source": [
    "#creating the cross validation datasets\n",
    "y = s_pandas['label'].to_numpy()\n",
    "y = y.reshape(-1,1)\n",
    "x = s_pandas.drop(columns=['label'])\n",
    "\n",
    "x_train, x1_test, y_train, y1_test = train_test_split(x, y, test_size=0.5, random_state=9)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d6fa78cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584, 80)\n",
      "(2584, 1)\n",
      "(1292, 80)\n",
      "(1292, 1)\n",
      "(1293, 80)\n",
      "(1293, 1)\n"
     ]
    }
   ],
   "source": [
    "#split into test and validation layer\n",
    "x_val, x_test, y_val, y_test = train_test_split(x1_test, y1_test, test_size=0.5, random_state=9)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f2f0fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         0     1    2     3    4    5    6     7    8    9  ...  70  71  72  \\\n",
      "2194  4640    14   91    48    0    0    0     0    0    0  ...   0   0   0   \n",
      "2259  4678   253   27  1122    0    0    0     0    0    0  ...   0   0   0   \n",
      "3644    65   487   20    58  986  394   36   798   14  107  ...   0   0   0   \n",
      "107      7  3025  433   199  416    8  204  1108  161   20  ...   0   0   0   \n",
      "2584   208    73  300  1270   43  500  202     0    0    0  ...   0   0   0   \n",
      "\n",
      "      73  74  75  76  77  78  \\\n",
      "2194   0   0   0   0   0   0   \n",
      "2259   0   0   0   0   0   0   \n",
      "3644   0   0   0   0   0   0   \n",
      "107    0   0   0   0   0   0   \n",
      "2584   0   0   0   0   0   0   \n",
      "\n",
      "                                                    doc  \n",
      "2194                                velli good ye pleas  \n",
      "2259                             nohe join today itself  \n",
      "3644      happi sad one thing past is it more good morn  \n",
      "107   know grumpi old peopl mom like better lie alwa...  \n",
      "2584                  yup alreadi thanx print n hand up  \n",
      "\n",
      "[5 rows x 80 columns]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#checking that i split this correctly\n",
    "print(x_train.head())\n",
    "print(y_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8fad549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584,)\n",
      "(1293,)\n",
      "(2584, 79)\n",
      "(1293, 79)\n"
     ]
    }
   ],
   "source": [
    "#splitting out the text doc from the encoded data\n",
    "train_doc = x_train['doc']\n",
    "test_doc = x_test['doc']\n",
    "val_doc = x_val['doc']\n",
    "x_train = x_train.drop(columns=['doc'])\n",
    "x_test = x_test.drop(columns=['doc'])\n",
    "x_val = x_val.drop(columns=['doc'])\n",
    "print(train_doc.shape)\n",
    "print(test_doc.shape)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d39bef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2584\n",
      "1293\n",
      "1292\n"
     ]
    }
   ],
   "source": [
    "#our data needs to be in array format so we conver the new dataframes back to arrays\n",
    "x_train_array = x_train.to_numpy()\n",
    "x_test_array = x_test.to_numpy()\n",
    "x_val_array = x_val.to_numpy()\n",
    "#and checking shaping\n",
    "print(len(x_train_array))\n",
    "print(len(x_test_array))\n",
    "print(len(x_val_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f3a4e987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584, 79)\n",
      "(1293, 79)\n",
      "(1292, 79)\n",
      "(2584, 1)\n",
      "(1293, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_array.shape)\n",
    "print(x_test_array.shape)\n",
    "print(x_val_array.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9da7c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving index of records that made it into test versus train\n",
    "train_index = x_train.index\n",
    "val_index = x_val.index\n",
    "test_index = x_test.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83780414",
   "metadata": {},
   "source": [
    "### One Hotkey Encoding (Count Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "999ca2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the one hotkey on clean documents\n",
    "# vec = CountVectorizer()\n",
    "# X_train_count = vec.fit_transform(df['documents_clean'].values)\n",
    "# X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3022dde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking shape\n",
    "# len(X_train_count.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c7bea37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#moving into pandas dataframe\n",
    "# df_one = pd.DataFrame(X_train_count.toarray())\n",
    "# len(df_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "edb0915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using index to split into train and test\n",
    "# df_one_train = df_one.iloc[train_index]\n",
    "# df_one_test = df_one.iloc[test_index]\n",
    "# print(df_one_train.shape)\n",
    "# print(df_one_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3d5027",
   "metadata": {},
   "source": [
    "### One Hotkey Encoding (TFID Vectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "438e84aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating the one hotkey on clean documents\n",
    "vec2 = TfidfVectorizer(max_features=7000)\n",
    "X_train_tfidf = vec2.fit_transform(df['documents_clean'].values)\n",
    "X_train_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "470dd0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5169"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking shape\n",
    "len(X_train_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "76bcdaa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5169, 6793)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#moving into pandas dataframe\n",
    "df_vec = pd.DataFrame(X_train_tfidf.toarray())\n",
    "df_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "05332e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2584, 6793)\n",
      "(1292, 6793)\n",
      "(1293, 6793)\n"
     ]
    }
   ],
   "source": [
    "#using index to split into train and test\n",
    "df_train = df_vec.iloc[train_index]\n",
    "df_test = df_vec.iloc[test_index]\n",
    "df_val = df_vec.iloc[val_index]\n",
    "print(df_train.shape)\n",
    "print(df_val.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d338840",
   "metadata": {},
   "source": [
    "### Defining Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8789c28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_metrics(method):\n",
    "    a = print(\"Accuracy:\",metrics.accuracy_score(y_test, method))\n",
    "    p = print(\"Precision:\",metrics.precision_score(y_test, method))\n",
    "    r =print(\"Recall:\",metrics.recall_score(y_test, method))\n",
    "    return a, p, r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae635a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_accuracy(method):\n",
    "    a = metrics.accuracy_score(y_test, method)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f6157e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_precision(method):\n",
    "    p = metrics.precision_score(y_test, method)\n",
    "    return p;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "49e48ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_recall(method):\n",
    "    r =metrics.recall_score(y_test, method)\n",
    "    return r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7ce520da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_f1(method):\n",
    "    r =metrics.f1_score(y_test, method)\n",
    "    return r;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "47f0ef8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_metrics(method):\n",
    "    a = print(\"Accuracy:\",metrics.accuracy_score(y_train, method))\n",
    "    p = print(\"Precision:\",metrics.precision_score(y_train, method))\n",
    "    r =print(\"Recall:\",metrics.recall_score(y_train, method))\n",
    "    return a, p, r;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed9fcbf",
   "metadata": {},
   "source": [
    "### Setting Up Tensors for each of the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "34ab550f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the tensor\n",
    "# ts_train = tf.data.Dataset.from_tensor_slices((x_train_array, y_train))\n",
    "# ts_holdout = tf.data.Dataset.from_tensor_slices((x_test_array, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0a738d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "#ts_train = ts_train.padded_batch(32, padded_shapes=([-1], []))\n",
    "#ts_holdout = ts_holdout.padded_batch(32, padded_shapes=([-1], []))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed395f80",
   "metadata": {},
   "source": [
    "### Establishing Early Stopping Criteria for NN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0b9077c",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = keras.callbacks.EarlyStopping(monitor='precision',patience=15,min_delta=.001,verbose=1,mode=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b60ae5",
   "metadata": {},
   "source": [
    "### Under Sampling Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bec65de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "under = RandomUnderSampler(sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164c1a9a",
   "metadata": {},
   "source": [
    "### Weighting Scheme for Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c8c519ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = {0: 1, 1: 8}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a69068",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9d78bc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"lbfgs\",random_state=9,warm_start=False,class_weight='balanced')\n",
    "lrm = lr.fit(df_train, y_train.ravel())\n",
    "lrr = lrm.predict(df_train)\n",
    "lrt = lrm.predict(df_test)\n",
    "lrv = lrm.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9e8f24b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9613302397525135\n",
      "Precision: 0.85\n",
      "Recall: 0.8395061728395061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(lrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "35b2f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.0, 'class_weight': 'balanced', 'dual': False, 'fit_intercept': True, 'intercept_scaling': 1, 'l1_ratio': None, 'max_iter': 100, 'multi_class': 'auto', 'n_jobs': None, 'penalty': 'l2', 'random_state': 9, 'solver': 'lbfgs', 'tol': 0.0001, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(lrm.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d425295",
   "metadata": {},
   "source": [
    "### Problems with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d1ca5",
   "metadata": {},
   "source": [
    "http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af6c20",
   "metadata": {},
   "source": [
    "Problems with Naive Bayes:\n",
    "    Doesnt handle imbalanced data well\n",
    "    Assumes feature indepedence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61db471",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "184850e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = GaussianNB()\n",
    "gn_steps = [('under', under), ('model', gn)]\n",
    "gn_pipeline = Pipeline(steps=gn_steps)\n",
    "gnm = gn_pipeline.fit(df_train, y_train)\n",
    "gnr = gnm.predict(df_train)\n",
    "gnt = gnm.predict(df_test)\n",
    "gnv = gnm.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0ecc663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8089713843774169\n",
      "Precision: 0.38605898123324395\n",
      "Recall: 0.8888888888888888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(gnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9dd5f191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('under', RandomUnderSampler(sampling_strategy=0.5)), ('model', GaussianNB())], 'verbose': False, 'under': RandomUnderSampler(sampling_strategy=0.5), 'model': GaussianNB(), 'under__random_state': None, 'under__replacement': False, 'under__sampling_strategy': 0.5, 'model__priors': None, 'model__var_smoothing': 1e-09}\n"
     ]
    }
   ],
   "source": [
    "print(gnm.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8828248",
   "metadata": {},
   "source": [
    "### Bernouli Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a65ed55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BernoulliNB()\n",
    "bn_steps = [('under', under), ('model', bn)]\n",
    "bn_pipeline = Pipeline(steps=bn_steps)\n",
    "bnm = bn_pipeline.fit(df_train, y_train)\n",
    "bnr = bnm.predict(df_train)\n",
    "bnt = bnm.predict(df_test)\n",
    "bnv = bnm.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8de1d700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9574632637277649\n",
      "Precision: 0.9908256880733946\n",
      "Recall: 0.6666666666666666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(bnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "41c17eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'binarize': 0.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': 'warn'}\n"
     ]
    }
   ],
   "source": [
    "print(bn.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56fd40",
   "metadata": {},
   "source": [
    "### MultiNomimail Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "54eb1b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = MultinomialNB()\n",
    "mb_steps = [('under', under), ('model', mb)]\n",
    "mb_pipeline = Pipeline(steps=mb_steps)\n",
    "mbm = mb_pipeline.fit(df_train, y_train)\n",
    "mbr = mbm.predict(df_train)\n",
    "mbt = mbm.predict(df_test)\n",
    "mbv = mbm.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "195345ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9690641918020109\n",
      "Precision: 0.9121621621621622\n",
      "Recall: 0.8333333333333334\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(mbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "530f4726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'class_prior': None, 'fit_prior': True, 'force_alpha': 'warn'}\n"
     ]
    }
   ],
   "source": [
    "print(mb.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6f4d6b",
   "metadata": {},
   "source": [
    "### Hypertuned SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f6aac007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_sv = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid','linear']}\n",
    "# grid_sv = GridSearchCV(svm.SVC(random_state=9,class_weight='balanced'),param_grid_sv,refit=True,verbose=3,scoring='precision',cv=5)\n",
    "# shm = grid_sv.fit(df_train,y_train.ravel())\n",
    "# print(grid_sv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a584d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svr = grid_sv.predict(df_train)\n",
    "# svt = grid_sv.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "eb582491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(svt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7e5483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sv_grid = pd.DataFrame(data=grid_sv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "524f3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sv_grid.to_excel(\"SVC_GridSearch.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0695fa30",
   "metadata": {},
   "source": [
    "###  Best Fit SVM (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "be3ed1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb = svm.SVC(kernel='poly',random_state=9,C=.1,gamma=1,class_weight='balanced')\n",
    "# sbm = sb.fit(df_train, y_train.ravel())\n",
    "# sbr = sbm.predict(df_train)\n",
    "# sbt = sbm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a28dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(sbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4ca507c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sbm.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc90336",
   "metadata": {},
   "source": [
    "### Best Fit SVM (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "78092cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb2 = svm.SVC(kernel='rbf',random_state=9,C=10,gamma=0.1,class_weight='balanced')\n",
    "sbm2 = sb2.fit(df_train, y_train.ravel())\n",
    "sbr2 = sbm2.predict(df_train)\n",
    "sbt2 = sbm2.predict(df_test)\n",
    "sbv2 = sbm2.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "845e280f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9675174013921114\n",
      "Precision: 0.9285714285714286\n",
      "Recall: 0.8024691358024691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(sbt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2f152c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10, 'break_ties': False, 'cache_size': 200, 'class_weight': 'balanced', 'coef0': 0.0, 'decision_function_shape': 'ovr', 'degree': 3, 'gamma': 0.1, 'kernel': 'rbf', 'max_iter': -1, 'probability': False, 'random_state': 9, 'shrinking': True, 'tol': 0.001, 'verbose': False}\n"
     ]
    }
   ],
   "source": [
    "print(sbm2.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68336e34",
   "metadata": {},
   "source": [
    "### Hyertuned Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "768d4647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 10)]\n",
    "# max_features = ['auto', 'sqrt']\n",
    "# max_depth = [int(x) for x in np.linspace(10, 1000, num = 11)]\n",
    "# max_depth.append(None)\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# bootstrap = [True, False]\n",
    "# random_grid = {\n",
    "#                'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                'bootstrap': bootstrap\n",
    "#                 }\n",
    "# print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59636522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(class_weight=\"balanced\",random_state=9)\n",
    "# rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 5, verbose=2, random_state=9, scoring='precision')\n",
    "# rf_random.fit(df_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "07e83b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(rf_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1a39cedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rhr = rf_random.predict(df_train)\n",
    "# rgt = rf_random.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f7684fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rf_grid = pd.DataFrame(data=rf_random.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "13b048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_rf_grid.to_excel(\"RF_GridSearch.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d2a2d",
   "metadata": {},
   "source": [
    "### Best Fit RF (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6d75fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(\n",
    "#     class_weight=\"balanced\",\n",
    "#     random_state=9,\n",
    "#     n_estimators=100,\n",
    "#     min_samples_split=2,\n",
    "#     min_samples_leaf=1,\n",
    "#     max_features='sqrt',\n",
    "#     max_depth=307,\n",
    "#     bootstrap=True)\n",
    "# rfm = rf.fit(df_train, y_train.ravel())\n",
    "# rfr = rfm.predict(df_train)\n",
    "# rft = rfm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d138796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(rft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "79d678e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at default parameters\n",
    "# print(rf.get_params())\n",
    "# print('Max Depth of any Tree: ',max([estimator.tree_.max_depth for estimator in rf.estimators_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15af368",
   "metadata": {},
   "source": [
    "### Best Fit RF (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fe729d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CGLam\\anaconda3_\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:424: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'` or remove this parameter as it is also the default value for RandomForestClassifiers and ExtraTreesClassifiers.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "rf2 = RandomForestClassifier(\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=9,\n",
    "    n_estimators=1577,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='auto',\n",
    "    max_depth=307,\n",
    "    bootstrap=False)\n",
    "rfm2 = rf2.fit(df_train, y_train.ravel())\n",
    "rfr2 = rfm2.predict(df_train)\n",
    "rft2 = rfm2.predict(df_test)\n",
    "rfv2 = rfm2.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d11b8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9791183294663574\n",
      "Precision: 0.9655172413793104\n",
      "Recall: 0.8641975308641975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(rft2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3a20363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 307, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 1577, 'n_jobs': None, 'oob_score': False, 'random_state': 9, 'verbose': 0, 'warm_start': False}\n",
      "Max Depth of any Tree:  307\n"
     ]
    }
   ],
   "source": [
    "#looking at default parameters\n",
    "print(rf2.get_params())\n",
    "print('Max Depth of any Tree: ',max([estimator.tree_.max_depth for estimator in rf2.estimators_]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125a45ac",
   "metadata": {},
   "source": [
    "### Hyertuned Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3a0fbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 500, num = 3)]\n",
    "# max_features = ['log2', 'sqrt']\n",
    "# max_depth = [int(x) for x in np.linspace(1, 10, num = 3)]\n",
    "# max_depth.append(None)\n",
    "# min_samples_split = [2, 5, 10]\n",
    "# min_samples_leaf = [1, 2, 4]\n",
    "# random_grid = {\n",
    "#                'n_estimators': n_estimators,\n",
    "#                'max_features': max_features,\n",
    "#                'max_depth': max_depth,\n",
    "#                'min_samples_split': min_samples_split,\n",
    "#                'min_samples_leaf': min_samples_leaf,\n",
    "#                 }\n",
    "# print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "680017bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb = GradientBoostingClassifier(random_state=9)\n",
    "# gb_grid = GridSearchCV(gb,random_grid, cv = 5, verbose=2, scoring='precision')\n",
    "# gb_grid.fit(df_train, y_train.ravel())\n",
    "# print(gb_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c9dcf22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gb_grid.best_params_)"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "27aad884",
   "metadata": {},
   "source": [
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "15c80cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gb_grid = pd.DataFrame(data=gb_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9bcceb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gb_grid.to_excel(\"Gradient_Boosted_GridSearch.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4762dee6",
   "metadata": {},
   "source": [
    "### Best Fit Gradient Boosted Trees (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a188c77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb = GradientBoostingClassifier(warm_start=False,\n",
    "#                                 random_state=9,\n",
    "#                                 max_depth=1,\n",
    "#                                 max_features='log2',\n",
    "#                                 min_samples_leaf=1,\n",
    "#                                 min_samples_split=2,\n",
    "#                                 n_estimators=100)\n",
    "                                \n",
    "# gbm = gb.fit(df_train, y_train.ravel())\n",
    "# gbr = gbm.predict(df_train)\n",
    "# gbt = gbm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "268e70b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(gbt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "53351838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(gb.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f5dd2c",
   "metadata": {},
   "source": [
    "### Best Fit Gradient Boosted Trees (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "605bc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb2 = GradientBoostingClassifier(warm_start=False,\n",
    "                                random_state=9,\n",
    "                                max_depth=None,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=4,\n",
    "                                min_samples_split=10,\n",
    "                                n_estimators=100)\n",
    "                                \n",
    "gbm2 = gb2.fit(df_train, y_train.ravel())\n",
    "gbr2 = gbm2.predict(df_train)\n",
    "gbt2 = gbm2.predict(df_test)\n",
    "gbv2 = gbm2.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2ad4f0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.974477958236659\n",
      "Precision: 0.9640287769784173\n",
      "Recall: 0.8271604938271605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(gbt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6e242667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 4, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_iter_no_change': None, 'random_state': 9, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(gb2.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc8143",
   "metadata": {},
   "source": [
    "### Model - Hyertuned ExtraTrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ce147392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators_e = [int(x) for x in np.linspace(start = 100, stop = 500, num = 3)]\n",
    "# max_features_e = ['log2', 'sqrt']\n",
    "# min_samples_split_e = [2, 5, 10]\n",
    "# min_samples_leaf_e = [1, 2, 4]\n",
    "# bootstrap_e = ['bool',False]\n",
    "# criterion_e = ['gini','log_loss','entropy']\n",
    "# random_grid_e = {\n",
    "#                'n_estimators': n_estimators_e,\n",
    "#                'max_features': max_features_e,\n",
    "#                'min_samples_split': min_samples_split_e,\n",
    "#                'min_samples_leaf': min_samples_leaf_e,\n",
    "#                 'bootstrap': bootstrap_e,\n",
    "#                 'criterion': criterion_e\n",
    "#                 }\n",
    "# print(random_grid_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6f2cdf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# et = ExtraTreesClassifier(warm_start=False,random_state=9,class_weight='Balanced')\n",
    "# et_grid = GridSearchCV(et,random_grid_e, cv = 5, verbose=2, scoring='precision')\n",
    "# et_grid.fit(df_train, y_train.ravel())\n",
    "# print(et_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4489a61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(et_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "481b3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_et_grid = pd.DataFrame(data=et_grid.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b718f322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_et_grid.to_excel(\"ET_GridSearch.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a160c21b",
   "metadata": {},
   "source": [
    "### Best Fit Extra Trees (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8bb4e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# et = ExtraTreesClassifier(\n",
    "#         warm_start=False,\n",
    "#         random_state=9,\n",
    "#         bootstrap=True,\n",
    "#         class_weight='balanced',\n",
    "#         criterion='log_loss',\n",
    "#         max_features='log2',\n",
    "#         min_samples_split=10)\n",
    "# etm = et.fit(df_train, y_train.ravel())\n",
    "# etr = etm.predict(df_train)\n",
    "# ett = etm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "95e2e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(ett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4c499c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(et.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d350408b",
   "metadata": {},
   "source": [
    "### Best Fit Extra Trees (F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a6408334",
   "metadata": {},
   "outputs": [],
   "source": [
    "et2 = ExtraTreesClassifier(\n",
    "        warm_start=False,\n",
    "        random_state=9,\n",
    "        bootstrap=False,\n",
    "        class_weight='balanced',\n",
    "        criterion='log_loss',\n",
    "        max_features='log2',\n",
    "        min_samples_split=10)\n",
    "etm2 = et2.fit(df_train, y_train.ravel())\n",
    "etr2 = etm2.predict(df_train)\n",
    "ett2 = etm2.predict(df_test)\n",
    "etv2 = etm2.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ea34702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9752513534416086\n",
      "Precision: 0.9710144927536232\n",
      "Recall: 0.8271604938271605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(ett2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "db0a8c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bootstrap': False, 'ccp_alpha': 0.0, 'class_weight': 'balanced', 'criterion': 'log_loss', 'max_depth': None, 'max_features': 'log2', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 10, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 9, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(et2.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782cf19",
   "metadata": {},
   "source": [
    "### Hypertuned ADA Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aed56b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid_ad = {'n_estimators': [int(x) for x in np.linspace(start = 20, stop = 300, num = 10)], \n",
    "#                  'learning_rate': [.1,.5,1]}\n",
    "# grid_ad = GridSearchCV(AdaBoostClassifier(random_state=9),param_grid_ad,refit=True,verbose=3,scoring='precision',cv=5)\n",
    "# adm = grid_ad.fit(df_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8568b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(grid_ad.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "eb3c9316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ad_grid = pd.DataFrame(data=grid_ad.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2147e57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ad_grid.to_excel(\"AD_GridSearch.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38b0d55",
   "metadata": {},
   "source": [
    "### Best Fit ADA Boosting (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81bc2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad = AdaBoostClassifier(random_state=9,learning_rate=0.1,n_estimators=20)\n",
    "# adm = ad.fit(df_train, y_train.ravel())\n",
    "# adr = adm.predict(df_train)\n",
    "# adt = adm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0e1582fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(adt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c5d7e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ad.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6de9b",
   "metadata": {},
   "source": [
    "### Best Fit ADA Bososting (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "19e45c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad2 = AdaBoostClassifier(random_state=9,learning_rate=0.5,n_estimators=268)\n",
    "adm2 = ad2.fit(df_train, y_train.ravel())\n",
    "adr2 = adm2.predict(df_train)\n",
    "adt2 = adm2.predict(df_test)\n",
    "adv2 = adm2.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "46f1890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9644238205723125\n",
      "Precision: 0.9027777777777778\n",
      "Recall: 0.8024691358024691\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(adt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "365933fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'algorithm': 'SAMME.R', 'base_estimator': 'deprecated', 'estimator': None, 'learning_rate': 0.5, 'n_estimators': 268, 'random_state': 9}\n"
     ]
    }
   ],
   "source": [
    "print(ad2.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98716a1",
   "metadata": {},
   "source": [
    "### XGB Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "655aff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {'max_depth': [3, 6, 10],\n",
    "#               'learning_rate': [ 0.1, 0.5],\n",
    "#               'subsample': np.arange(0.5, 1.0, 0.2),\n",
    "#               'colsample_bytree': np.arange(0.5, 1.0, 0.2),\n",
    "#               'colsample_bylevel': np.arange(0.5, 1.0, 0.2),\n",
    "#               'n_estimators': [50, 100],\n",
    "#               'num_class': [1]\n",
    "#               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f9bd062a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid_xg = GridSearchCV(XGBClassifier(random_state=9,scale_pos_weight=20),params,refit=True,verbose=3,scoring='precision',cv=5,n_jobs=-2)\n",
    "# xgm = grid_xg.fit(df_train,y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dfd40d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(grid_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "91394905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_xg_grid = pd.DataFrame(data=grid_xg.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "64a6eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_xg_grid.to_excel(\"XG_GridSearch.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9903dd",
   "metadata": {},
   "source": [
    "### Model - XGB Classifier (precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "06e05764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xg = XGBClassifier(\n",
    "#                     random_state=9,\n",
    "#                     colsample_bylevel=0.5, \n",
    "#                     colsample_bytree=0.7, \n",
    "#                     learning_rate=0.5, \n",
    "#                     max_depth=6, \n",
    "#                     n_estimators=100, \n",
    "#                     num_class=1, \n",
    "#                     subsample=0.9,\n",
    "#                     scale_pos_weight=20)\n",
    "# xgm = xg.fit(df_train, y_train.ravel())\n",
    "# xgr = xgm.predict(df_train)\n",
    "# xgt = xgm.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "03d93d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(xgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7d2333ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(xg.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0a90b3",
   "metadata": {},
   "source": [
    "### Best Fit XGB Classifier (f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "76029828",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg2 = XGBClassifier(\n",
    "                    random_state=9,\n",
    "                    colsample_bylevel=0.5, \n",
    "                    colsample_bytree=0.5, \n",
    "                    learning_rate=0.1, \n",
    "                    max_depth=10, \n",
    "                    n_estimators=250, \n",
    "                    num_class=1, \n",
    "                    subsample=0.75,\n",
    "                    scale_pos_weight=20)\n",
    "xgm2 = xg2.fit(df_train, y_train.ravel())\n",
    "xgr2 = xgm2.predict(df_train)\n",
    "xgt2 = xgm2.predict(df_test)\n",
    "xgv2 = xgm2.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c22a50f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9675174013921114\n",
      "Precision: 0.8658536585365854\n",
      "Recall: 0.8765432098765432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(xgt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0def667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': 0.5, 'colsample_bynode': None, 'colsample_bytree': 0.5, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': 10, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 250, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': 9, 'reg_alpha': None, 'reg_lambda': None, 'sampling_method': None, 'scale_pos_weight': 20, 'subsample': 0.75, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'num_class': 1}\n"
     ]
    }
   ],
   "source": [
    "print(xg2.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d1952",
   "metadata": {},
   "source": [
    "### Model XGBRF Classifier Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d0b63cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_brf = {'max_depth': [3, 6, 10, 15],\n",
    "#               'learning_rate': [ 0.1, 0.5, 1],\n",
    "#               'subsample': np.arange(0.5, 1.0, 0.1),\n",
    "#               'colsample_bytree': np.arange(0.5, 1.0, 0.1),\n",
    "#               'colsample_bylevel': np.arange(0.5, 1.0, 0.1),\n",
    "#               'n_estimators': [50, 100, 250],\n",
    "#               'num_class': [1]\n",
    "#               }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc59b5f0",
   "metadata": {},
   "source": [
    "### Model - XGBRF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "02cedc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr = XGBRFClassifier(random_state=9)\n",
    "xrm = xr.fit(df_train, y_train)\n",
    "xrr = xrm.predict(df_train)\n",
    "xrt = xrm.predict(df_test)\n",
    "xrv = xrm.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e0073f61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9404485692188709\n",
      "Precision: 0.8346456692913385\n",
      "Recall: 0.654320987654321\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(xrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9dfdec04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bynode': 0.8, 'learning_rate': 1.0, 'reg_lambda': 1e-05, 'subsample': 0.8, 'objective': 'binary:logistic', 'use_label_encoder': None, 'base_score': None, 'booster': None, 'callbacks': None, 'colsample_bylevel': None, 'colsample_bytree': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': None, 'gpu_id': None, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': None, 'max_depth': None, 'max_leaves': None, 'min_child_weight': None, 'missing': nan, 'monotone_constraints': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'predictor': None, 'random_state': 9, 'reg_alpha': None, 'sampling_method': None, 'scale_pos_weight': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None}\n"
     ]
    }
   ],
   "source": [
    "print(xr.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1a2fb",
   "metadata": {},
   "source": [
    "### Model_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c5d51b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 79, 64)            436480    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 64)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 442,753\n",
      "Trainable params: 442,753\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "md_cnn = tf.keras.Sequential()\n",
    "md_cnn.add(tf.keras.layers.Embedding(input_dim=len(d)+1, output_dim=64, input_length=x_train.shape[1]))\n",
    "md_cnn.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "md_cnn.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "md_cnn.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "md_cnn.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\n",
    "md_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "md_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "8c190ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "81/81 [==============================] - 1s 3ms/step - loss: 1.3179 - accuracy: 0.1393 - precision: 0.1301 - recall: 0.9852 \n",
      "Epoch 2/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 1.1210 - accuracy: 0.6865 - precision: 0.2890 - recall: 0.9614\n",
      "Epoch 3/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.9830 - precision: 0.9198 - recall: 0.9525\n",
      "Epoch 4/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9834 - precision: 0.9129 - recall: 0.9644\n",
      "Epoch 5/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.1218 - accuracy: 0.9849 - precision: 0.9209 - recall: 0.9674\n",
      "Epoch 6/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9903 - precision: 0.9432 - recall: 0.9852\n",
      "Epoch 7/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0705 - accuracy: 0.9930 - precision: 0.9650 - recall: 0.9822\n",
      "Epoch 8/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0534 - accuracy: 0.9903 - precision: 0.9457 - recall: 0.9822\n",
      "Epoch 9/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0588 - accuracy: 0.9907 - precision: 0.9459 - recall: 0.9852\n",
      "Epoch 10/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0427 - accuracy: 0.9957 - precision: 0.9766 - recall: 0.9911\n",
      "Epoch 11/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0464 - accuracy: 0.9919 - precision: 0.9489 - recall: 0.9911\n",
      "Epoch 12/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0421 - accuracy: 0.9942 - precision: 0.9680 - recall: 0.9881\n",
      "Epoch 13/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0349 - accuracy: 0.9946 - precision: 0.9681 - recall: 0.9911\n",
      "Epoch 14/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0217 - accuracy: 0.9961 - precision: 0.9739 - recall: 0.9970\n",
      "Epoch 15/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0393 - accuracy: 0.9926 - precision: 0.9543 - recall: 0.9911\n",
      "Epoch 16/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0290 - accuracy: 0.9961 - precision: 0.9767 - recall: 0.9941\n",
      "Epoch 17/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0347 - accuracy: 0.9950 - precision: 0.9682 - recall: 0.9941\n",
      "Epoch 18/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9965 - precision: 0.9795 - recall: 0.9941\n",
      "Epoch 19/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0231 - accuracy: 0.9969 - precision: 0.9824 - recall: 0.9941\n",
      "Epoch 20/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0265 - accuracy: 0.9973 - precision: 0.9853 - recall: 0.9941\n",
      "Epoch 21/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0266 - accuracy: 0.9950 - precision: 0.9655 - recall: 0.9970\n",
      "Epoch 22/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0256 - accuracy: 0.9961 - precision: 0.9767 - recall: 0.9941\n",
      "Epoch 23/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0254 - accuracy: 0.9969 - precision: 0.9824 - recall: 0.9941\n",
      "Epoch 24/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0228 - accuracy: 0.9973 - precision: 0.9825 - recall: 0.9970\n",
      "Epoch 25/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0187 - accuracy: 0.9969 - precision: 0.9796 - recall: 0.9970\n",
      "Epoch 26/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9973 - precision: 0.9825 - recall: 0.9970\n",
      "Epoch 27/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0132 - accuracy: 0.9969 - precision: 0.9796 - recall: 0.9970\n",
      "Epoch 28/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0263 - accuracy: 0.9957 - precision: 0.9738 - recall: 0.9941\n",
      "Epoch 29/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0112 - accuracy: 0.9992 - precision: 0.9970 - recall: 0.9970\n",
      "Epoch 30/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9973 - precision: 0.9797 - recall: 1.0000\n",
      "Epoch 31/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0192 - accuracy: 0.9957 - precision: 0.9711 - recall: 0.9970\n",
      "Epoch 32/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0303 - accuracy: 0.9942 - precision: 0.9626 - recall: 0.9941\n",
      "Epoch 33/50\n",
      "81/81 [==============================] - 0s 3ms/step - loss: 0.0208 - accuracy: 0.9954 - precision: 0.9710 - recall: 0.9941\n",
      "Epoch 34/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9977 - precision: 0.9853 - recall: 0.9970\n",
      "Epoch 35/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.9961 - precision: 0.9739 - recall: 0.9970\n",
      "Epoch 36/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0185 - accuracy: 0.9965 - precision: 0.9767 - recall: 0.9970\n",
      "Epoch 37/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9977 - precision: 0.9853 - recall: 0.9970\n",
      "Epoch 38/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.9965 - precision: 0.9767 - recall: 0.9970\n",
      "Epoch 39/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9965 - precision: 0.9767 - recall: 0.9970\n",
      "Epoch 40/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9969 - precision: 0.9796 - recall: 0.9970\n",
      "Epoch 41/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9969 - precision: 0.9796 - recall: 0.9970\n",
      "Epoch 42/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9985 - precision: 0.9912 - recall: 0.9970\n",
      "Epoch 43/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0180 - accuracy: 0.9965 - precision: 0.9767 - recall: 0.9970\n",
      "Epoch 44/50\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 0.9977 - precision: 0.9853 - recall: 0.9970\n",
      "Epoch 44: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15dffb85060>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_cnn.fit(x_train_array, y_train, epochs=50, callbacks=[callback],class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "b9db7579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 0s 920us/step\n",
      "81/81 [==============================] - 0s 1ms/step\n",
      "41/41 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "cnt = md_cnn.predict(x_test_array, batch_size=None)\n",
    "cnr = md_cnn.predict(x_train_array, batch_size=None)\n",
    "cnv = md_cnn.predict(x_val_array, batch_size=None)\n",
    "cnt = np.concatenate(cnt).round().astype(int)\n",
    "cnr = np.concatenate(cnr).round().astype(int)\n",
    "cnv = np.concatenate(cnv).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5a105c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9667440061871616\n",
      "Precision: 0.9280575539568345\n",
      "Recall: 0.7962962962962963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7097b6",
   "metadata": {},
   "source": [
    "### Model - BiDirectional LSTM with Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "a19ce512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 79, 100)           682000    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 128)              84480     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 776,849\n",
      "Trainable params: 776,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#embedding layer and bidirectional layer\n",
    "md_bidir_w_hidden = tf.keras.Sequential()\n",
    "md_bidir_w_hidden.add(tf.keras.layers.Embedding(input_dim= len(d) + 1,output_dim=100,input_length=x_train.shape[1]))\n",
    "md_bidir_w_hidden.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "md_bidir_w_hidden.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "md_bidir_w_hidden.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "md_bidir_w_hidden.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "md_bidir_w_hidden.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "6a619f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\n",
    "md_bidir_w_hidden.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "2fce96b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "81/81 [==============================] - 6s 50ms/step - loss: 1.1727 - accuracy: 0.4234 - precision: 0.1737 - recall: 0.9110\n",
      "Epoch 2/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.7938 - accuracy: 0.7988 - precision: 0.3798 - recall: 0.8576\n",
      "Epoch 3/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.3650 - accuracy: 0.9431 - precision: 0.7179 - recall: 0.9288\n",
      "Epoch 4/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.2817 - accuracy: 0.9567 - precision: 0.7685 - recall: 0.9555\n",
      "Epoch 5/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.1978 - accuracy: 0.9690 - precision: 0.8237 - recall: 0.9703\n",
      "Epoch 6/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.1398 - accuracy: 0.9772 - precision: 0.8639 - recall: 0.9792\n",
      "Epoch 7/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0899 - accuracy: 0.9841 - precision: 0.9022 - recall: 0.9852\n",
      "Epoch 8/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0513 - accuracy: 0.9888 - precision: 0.9302 - recall: 0.9881\n",
      "Epoch 9/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0273 - accuracy: 0.9946 - precision: 0.9654 - recall: 0.9941\n",
      "Epoch 10/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0472 - accuracy: 0.9919 - precision: 0.9489 - recall: 0.9911\n",
      "Epoch 11/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0346 - accuracy: 0.9907 - precision: 0.9408 - recall: 0.9911\n",
      "Epoch 12/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0380 - accuracy: 0.9923 - precision: 0.9465 - recall: 0.9970\n",
      "Epoch 13/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0159 - accuracy: 0.9954 - precision: 0.9656 - recall: 1.0000\n",
      "Epoch 14/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0384 - accuracy: 0.9911 - precision: 0.9410 - recall: 0.9941\n",
      "Epoch 15/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0152 - accuracy: 0.9957 - precision: 0.9684 - recall: 1.0000\n",
      "Epoch 16/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0091 - accuracy: 0.9981 - precision: 0.9854 - recall: 1.0000\n",
      "Epoch 17/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0047 - accuracy: 0.9988 - precision: 0.9912 - recall: 1.0000\n",
      "Epoch 18/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0041 - accuracy: 0.9988 - precision: 0.9912 - recall: 1.0000\n",
      "Epoch 19/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0032 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 20/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0027 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 21/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0027 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 22/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0028 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 23/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0025 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 24/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0023 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 25/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0023 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 26/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0021 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 27/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0023 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 28/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0022 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 29/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0022 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 30/50\n",
      "81/81 [==============================] - 4s 49ms/step - loss: 0.0022 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 31/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0023 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 32/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0021 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 33/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0024 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 34/50\n",
      "81/81 [==============================] - 4s 50ms/step - loss: 0.0021 - accuracy: 0.9992 - precision: 0.9941 - recall: 1.0000\n",
      "Epoch 34: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15d92337220>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "md_bidir_w_hidden.fit(x_train_array, y_train, epochs=50, callbacks=[callback],class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "877eab93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41/41 [==============================] - 1s 16ms/step\n",
      "81/81 [==============================] - 1s 16ms/step\n",
      "41/41 [==============================] - 1s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "ltt = md_bidir_w_hidden.predict(x_test_array, batch_size=None)\n",
    "ltr = md_bidir_w_hidden.predict(x_train_array, batch_size=None)\n",
    "ltv = md_bidir_w_hidden.predict(x_val_array, batch_size=None)\n",
    "ltt = np.concatenate(ltt).round().astype(int)\n",
    "ltr = np.concatenate(ltr).round().astype(int)\n",
    "ltv = np.concatenate(ltv).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "17951e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9644238205723125\n",
      "Precision: 0.8717948717948718\n",
      "Recall: 0.8395061728395061\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(ltt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e99a25cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(data=ltt)\n",
    "out['actual'] = y_test\n",
    "out.to_excel('ltr.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc10875",
   "metadata": {},
   "source": [
    "### Model - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "eab5e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = [tf.keras.metrics.BinaryAccuracy(name='accuracy'),tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0b7379f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_df = pd.DataFrame(data=y_train,columns=['y'])\n",
    "# bert_df['Doc'] = train_doc.tolist()\n",
    "# print(bert_df.head())\n",
    "# print(bert_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f067b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_df['y'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0e81df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spam_df = bert_df[bert_df['y'] == 1]\n",
    "# print(spam_df.shape)\n",
    "# ham_df = bert_df[bert_df['y'] == 0]\n",
    "# print(ham_df.shape)\n",
    "# ham_downsample_df = ham_df.sample(spam_df.shape[0])\n",
    "# print(ham_downsample_df.shape)\n",
    "# test_balanced_df = pd.concat([spam_df,ham_downsample_df])\n",
    "# print(test_balanced_df.shape)\n",
    "# # print(test_balanced_df['y'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "01ca4908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_doc_balanced = test_balanced_df['Doc']\n",
    "# y_test_balanced = test_balanced_df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "14092cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "58a70a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "# bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4060b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Functional Model\n",
    "\n",
    "# # Bert layers\n",
    "# #Text input passed to bert_preprocess\n",
    "# text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "# preprocessed_text = bert_preprocess(text_input)\n",
    "# #Processed text passed to bert_encoder\n",
    "# outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# # Neural network layers\n",
    "# l = tf.keras.layers.Dense(64, activation='relu')\n",
    "# l = tf.keras.layers.Dense(32, activation='relu')\n",
    "# l = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "# #Dropout\n",
    "# l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "# l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# # Use inputs and outputs to construct a final model\n",
    "# model_bert = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d14f54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "09498173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bert.compile(optimizer='adam',loss='binary_crossentropy',metrics=METRICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7192dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_bert.fit(test_doc_balanced, y_test_balanced, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "94980f3a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_predict \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_bert\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(test_doc)\n\u001b[0;32m      2\u001b[0m test_predict \u001b[38;5;241m=\u001b[39m test_predict\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_bert' is not defined"
     ]
    }
   ],
   "source": [
    "# test_predict = model_bert.predict(test_doc)\n",
    "# test_predict = test_predict.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6403a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predict = test_predict.round().astype(int)\n",
    "# test_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e01fc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22748aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_f1(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e986eb8",
   "metadata": {},
   "source": [
    "### Model - Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dense, Input\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# import transformers\n",
    "# from tqdm.notebook import tqdm\n",
    "# from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# def bert_encode(data, maximum_length) :\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "\n",
    "#     for text in data:\n",
    "#         encoded = tokenizer.encode_plus(\n",
    "#             text, \n",
    "#             add_special_tokens=True,\n",
    "#             max_length=maximum_length,\n",
    "#             pad_to_max_length=True,\n",
    "\n",
    "#             return_attention_mask=True,\n",
    "#         )\n",
    "#         input_ids.append(encoded['input_ids'])\n",
    "#         attention_masks.append(encoded['attention_mask'])\n",
    "        \n",
    "#     return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c29ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = test_doc_balanced\n",
    "# target = y_test_balanced\n",
    "\n",
    "# train_input_ids, train_attention_masks = bert_encode(texts,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064591e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# def create_model(bert_model):\n",
    "    \n",
    "#     input_ids = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "#     attention_masks = tf.keras.Input(shape=(60,),dtype='int32')\n",
    "\n",
    "#     output = bert_model([input_ids,attention_masks])\n",
    "#     output = output[1]\n",
    "#     output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
    "#     output = tf.keras.layers.Dropout(0.2)(output)\n",
    "#     output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "    \n",
    "#     model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n",
    "#     model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2395f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import TFBertModel\n",
    "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912738c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model(bert_model)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = model.fit(\n",
    "#     [train_input_ids, train_attention_masks],\n",
    "#     target,\n",
    "#     validation_split=0.2, \n",
    "#     epochs=2,\n",
    "#     batch_size=10\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6b617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b2_test_predict = model_bert.predict(test_doc)\n",
    "# b2_test_predict = b2_test_predict.round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73acef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_metrics(b2_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f6b874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_f1(b2_test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958aade",
   "metadata": {},
   "source": [
    "### Model Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9191eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer,TFBertModel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2f0bde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, maxlen):\n",
    "    input_ids=[]\n",
    "    attention_masks=[]\n",
    "\n",
    "    for row in text:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            row,\n",
    "            add_special_tokens=True,\n",
    "            max_length=maxlen,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    return np.array(input_ids),np.array(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "420cb32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "C:\\Users\\CGLam\\anaconda3_\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_input_ids, X_train_attention_masks = encode(train_doc.values, maxlen=64)\n",
    "X_test_input_ids, X_test_attention_masks = encode(test_doc.values, maxlen=64)\n",
    "X_val_input_ids, X_val_attention_masks = encode(val_doc.values, maxlen=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f9c78a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_model):\n",
    "    input_word_ids = tf.keras.Input(shape=(64,),dtype='int32')\n",
    "    attention_masks = tf.keras.Input(shape=(64,),dtype='int32')\n",
    "\n",
    "    sequence_output = bert_model([input_word_ids,attention_masks])\n",
    "    output = sequence_output[1]\n",
    "    output = tf.keras.layers.Dense(32,activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    output = tf.keras.layers.Dense(1,activation='sigmoid')(output)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs = [input_word_ids,attention_masks], outputs = output)\n",
    "    model.compile(Adam(lr=1e-5),loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "39373321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 64)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_2[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 64,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           24608       ['tf_bert_model[0][1]']          \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 32)           0           ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            33          ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,506,881\n",
      "Trainable params: 109,506,881\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CGLam\\anaconda3_\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model3 = build_model(bert_model)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7f29fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "81/81 [==============================] - 263s 3s/step - loss: 0.7044 - accuracy: 0.8135 - val_loss: 0.3406 - val_accuracy: 0.8824\n",
      "Epoch 2/5\n",
      "81/81 [==============================] - 253s 3s/step - loss: 0.3494 - accuracy: 0.9396 - val_loss: 0.1212 - val_accuracy: 0.9598\n",
      "Epoch 3/5\n",
      "81/81 [==============================] - 249s 3s/step - loss: 0.2234 - accuracy: 0.9690 - val_loss: 0.0941 - val_accuracy: 0.9714\n",
      "Epoch 4/5\n",
      "81/81 [==============================] - 248s 3s/step - loss: 0.1472 - accuracy: 0.9807 - val_loss: 0.1406 - val_accuracy: 0.9582\n",
      "Epoch 5/5\n",
      "81/81 [==============================] - 250s 3s/step - loss: 0.1089 - accuracy: 0.9876 - val_loss: 0.0721 - val_accuracy: 0.9753\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(\n",
    "    [X_train_input_ids, X_train_attention_masks],\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=5,\n",
    "    validation_data=([X_test_input_ids, X_test_attention_masks], y_test),\n",
    "    class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "8b6d3866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 66s 817ms/step\n",
      "41/41 [==============================] - 33s 800ms/step\n",
      "41/41 [==============================] - 33s 806ms/step\n"
     ]
    }
   ],
   "source": [
    "ber = model3.predict([X_train_input_ids, X_train_attention_masks], batch_size=None)\n",
    "ber = np.concatenate(ber).round().astype(int)\n",
    "bet = model3.predict([X_test_input_ids, X_test_attention_masks], batch_size=None)\n",
    "bet = np.concatenate(bet).round().astype(int)\n",
    "bev = model3.predict([X_val_input_ids, X_val_attention_masks], batch_size=None)\n",
    "bev = np.concatenate(bev).round().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "2dd6548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9752513534416086\n",
      "Precision: 0.9166666666666666\n",
      "Recall: 0.8827160493827161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_metrics(bet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "ec7831ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ber = ber.tolist()\n",
    "bet = bet.tolist()\n",
    "bev = bev.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad5c12",
   "metadata": {},
   "source": [
    "### Model - BiDirectional LSTM with Convuluational Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e8768643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding layer and bidirectional layer\n",
    "#md_combo = tf.keras.Sequential()\n",
    "#md_combo.add(tf.keras.layers.Embedding(input_dim= len(d) + 1,output_dim=64,input_length=x_train.shape[1]))\n",
    "#md_combo.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
    "#md_combo.add(tf.keras.layers.GlobalAveragePooling2D(64,input_shape=(3,32,32)))\n",
    "#md_combo.add(Flatten())\n",
    "#md_combo.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "#md_combo.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "#md_combo.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "#md_combo.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dd14b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "#optimizer = keras.optimizers.SGD(learning_rate=0.02, momentum = 0.95, nesterov=True)\n",
    "#md_combo.compile(loss=\"binary_crossentropy\",optimizer=optimizer,metrics=['accuracy', 'Precision', 'Recall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "ad9b1244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model\n",
    "#md_combo.fit(x_train_array, y_train, epochs=50, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056839e8",
   "metadata": {},
   "source": [
    "### Report (Precision Hyptertuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "0f6aad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods = [lrt, gnt, bnt, mbt, sbt, rft, gbt, ett, adt, xgt, cnt, ltt]\n",
    "# method_name = ['LG','GNB','BNB','MNB','SVM','RF','GBT','ET','ADA','XGB','CNN','LTSM']\n",
    "# accuracy = []\n",
    "# precision = []\n",
    "# recall = []\n",
    "# f1 = []\n",
    "# for i in methods:\n",
    "#     accuracy.append(class_accuracy(i))\n",
    "#     precision.append(class_precision(i))\n",
    "#     recall.append(class_recall(i))\n",
    "#     f1.append(class_f1(i))\n",
    "# results = pd.DataFrame(\n",
    "#     {'Method': method_name,\n",
    "#      'Accuracy': accuracy,\n",
    "#      'Precision': precision,\n",
    "#      'Recall':recall,\n",
    "#      'F1':f1\n",
    "#     })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1f9ecc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1411ff02",
   "metadata": {},
   "source": [
    "### 3 Approach Voting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ea4f9e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_prediction(method1,method2,method3):\n",
    "    pred = []\n",
    "    for i in range(len(method1)):\n",
    "        pred.append(round((method1[i]+method2[i]+method3[i])/3))\n",
    "    return pred;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "43611f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vote_prediction5(method1,method2,method3,method4,method5):\n",
    "    pred = []\n",
    "    for i in range(len(method1)):\n",
    "        pred.append(round((method1[i]+method2[i]+method3[i]+method4[i]+method5[i])/5))\n",
    "    return pred;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f75ead1",
   "metadata": {},
   "source": [
    "### Report (Train Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "0d9d677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods1 = [lrr,mbr,sbr2,rfr2,gbr2,etr2,adr2,xgr2,xrr,cnr,ltr,ber]\n",
    "method_name1 = ['Logistic Regression','Nave Bayes','SVM','RF','Gradient Boosted','ET','Adaptive Boosting','XGBoost','RF with XGBoost','CNN','LTSM','BERT']\n",
    "accuracy1 = []\n",
    "precision1 = []\n",
    "recall1 = []\n",
    "f11 = []\n",
    "for i in methods1:\n",
    "    accuracy1.append(metrics.accuracy_score(y_train,i))\n",
    "    precision1.append(metrics.precision_score(y_train,i))\n",
    "    recall1.append(metrics.recall_score(y_train,i))\n",
    "    f11.append(metrics.f1_score(y_train,i))\n",
    "results1 = pd.DataFrame(\n",
    "    {'Method': method_name1,\n",
    "     'Accuracy': accuracy1,\n",
    "     'Precision': precision1,\n",
    "     'Recall':recall1,\n",
    "     'F1':f11\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "970fc454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.989164</td>\n",
       "      <td>0.935211</td>\n",
       "      <td>0.985163</td>\n",
       "      <td>0.959538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes-Gaussian</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.497784</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.664694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes-Bernouli</td>\n",
       "      <td>0.971749</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.783383</td>\n",
       "      <td>0.878536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes-MultiNomial</td>\n",
       "      <td>0.986455</td>\n",
       "      <td>0.966049</td>\n",
       "      <td>0.928783</td>\n",
       "      <td>0.947050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.997033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.998514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient-Boosted Trees</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994065</td>\n",
       "      <td>0.997024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Extremely Randomized Trees</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.998514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adapative Boosting</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.998514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.997678</td>\n",
       "      <td>0.985337</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.991150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Random Forest with XGBoostCNN</td>\n",
       "      <td>0.999613</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997033</td>\n",
       "      <td>0.998514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LTSM</td>\n",
       "      <td>0.999226</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.997041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.996904</td>\n",
       "      <td>0.985251</td>\n",
       "      <td>0.991098</td>\n",
       "      <td>0.988166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Method  Accuracy  Precision    Recall        F1\n",
       "0             Logistic Regression  0.989164   0.935211  0.985163  0.959538\n",
       "1            Naive Bayes-Gaussian  0.868421   0.497784  1.000000  0.664694\n",
       "2            Naive Bayes-Bernouli  0.971749   1.000000  0.783383  0.878536\n",
       "3         Naive Bayes-MultiNomial  0.986455   0.966049  0.928783  0.947050\n",
       "4                             SVM  0.999226   0.997033  0.997033  0.997033\n",
       "5                              RF  0.999613   1.000000  0.997033  0.998514\n",
       "6          Gradient-Boosted Trees  0.999226   1.000000  0.994065  0.997024\n",
       "7      Extremely Randomized Trees  0.999613   1.000000  0.997033  0.998514\n",
       "8              Adapative Boosting  0.999613   1.000000  0.997033  0.998514\n",
       "9                         XGBoost  0.997678   0.985337  0.997033  0.991150\n",
       "10  Random Forest with XGBoostCNN  0.999613   1.000000  0.997033  0.998514\n",
       "11                           LTSM  0.999226   0.994100  1.000000  0.997041\n",
       "12                           BERT  0.996904   0.985251  0.991098  0.988166"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9075d9c2",
   "metadata": {},
   "source": [
    "### Report (Val Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6956dd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods2 = [lrv,mbv,sbv2,rfv2,gbv2,etv2,adv2,xgv2,xrv,cnv,ltv,bev]\n",
    "method_name2 = ['Logistic Regression','Nave Bayes','SVM','RF','Gradient Boosted','ET','Adaptive Boosting','XGBoost','RF with XGBoost','CNN','LTSM','BERT']\n",
    "accuracy2 = []\n",
    "precision2 = []\n",
    "recall2 = []\n",
    "f12 = []\n",
    "for i in methods2:\n",
    "    accuracy2.append(metrics.accuracy_score(y_val,i))\n",
    "    precision2.append(metrics.precision_score(y_val,i))\n",
    "    recall2.append(metrics.recall_score(y_val,i))\n",
    "    f12.append(metrics.f1_score(y_val,i))\n",
    "results2 = pd.DataFrame(\n",
    "    {'Method': method_name2,\n",
    "     'Accuracy': accuracy2,\n",
    "     'Precision': precision2,\n",
    "     'Recall':recall2,\n",
    "     'F1':f12\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "bff48e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.965944</td>\n",
       "      <td>0.861842</td>\n",
       "      <td>0.850649</td>\n",
       "      <td>0.856209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes-Gaussian</td>\n",
       "      <td>0.835139</td>\n",
       "      <td>0.412463</td>\n",
       "      <td>0.902597</td>\n",
       "      <td>0.566191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes-Bernouli</td>\n",
       "      <td>0.966718</td>\n",
       "      <td>0.991150</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.838951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes-MultiNomial</td>\n",
       "      <td>0.978328</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.904762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.976780</td>\n",
       "      <td>0.969697</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.895105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.979876</td>\n",
       "      <td>0.963768</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.910959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gradient-Boosted Trees</td>\n",
       "      <td>0.980650</td>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.870130</td>\n",
       "      <td>0.914676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Extremely Randomized Trees</td>\n",
       "      <td>0.977554</td>\n",
       "      <td>0.949640</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.901024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Adapative Boosting</td>\n",
       "      <td>0.974458</td>\n",
       "      <td>0.941606</td>\n",
       "      <td>0.837662</td>\n",
       "      <td>0.886598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.943498</td>\n",
       "      <td>0.818898</td>\n",
       "      <td>0.675325</td>\n",
       "      <td>0.740214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.981424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844156</td>\n",
       "      <td>0.915493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LTSM</td>\n",
       "      <td>0.973684</td>\n",
       "      <td>0.922535</td>\n",
       "      <td>0.850649</td>\n",
       "      <td>0.885135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.974458</td>\n",
       "      <td>0.929078</td>\n",
       "      <td>0.850649</td>\n",
       "      <td>0.888136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Method  Accuracy  Precision    Recall        F1\n",
       "0          Logistic Regression  0.965944   0.861842  0.850649  0.856209\n",
       "1         Naive Bayes-Gaussian  0.835139   0.412463  0.902597  0.566191\n",
       "2         Naive Bayes-Bernouli  0.966718   0.991150  0.727273  0.838951\n",
       "3      Naive Bayes-MultiNomial  0.978328   0.950000  0.863636  0.904762\n",
       "4                          SVM  0.976780   0.969697  0.831169  0.895105\n",
       "5                           RF  0.979876   0.963768  0.863636  0.910959\n",
       "6       Gradient-Boosted Trees  0.980650   0.964029  0.870130  0.914676\n",
       "7   Extremely Randomized Trees  0.977554   0.949640  0.857143  0.901024\n",
       "8           Adapative Boosting  0.974458   0.941606  0.837662  0.886598\n",
       "9                      XGBoost  0.943498   0.818898  0.675325  0.740214\n",
       "10                         CNN  0.981424   1.000000  0.844156  0.915493\n",
       "11                        LTSM  0.973684   0.922535  0.850649  0.885135\n",
       "12                        BERT  0.974458   0.929078  0.850649  0.888136"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f450bab8",
   "metadata": {},
   "source": [
    "### Report (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "7895fa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods3 = [lrt,mbt,sbt2,rft2,gbt2,ett2,adt2,xgt2,xrt,cnt,ltt,bet]\n",
    "method_name3 = ['Logistic Regression','Nave Bayes','SVM','RF','Gradient Boosted','ET','Adaptive Boosting','XGBoost','RF with XGBoost','CNN','LTSM','BERT']\n",
    "accuracy3 = []\n",
    "precision3 = []\n",
    "recall3 = []\n",
    "f13 = []\n",
    "for i in methods3:\n",
    "    accuracy3.append(class_accuracy(i))\n",
    "    precision3.append(class_precision(i))\n",
    "    recall3.append(class_recall(i))\n",
    "    f13.append(class_f1(i))\n",
    "results3 = pd.DataFrame(\n",
    "    {'Method': method_name3,\n",
    "     'Accuracy': accuracy3,\n",
    "     'Precision': precision3,\n",
    "     'Recall':recall3,\n",
    "     'F1':f13\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "2a80b2dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.961330</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.844720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nave Bayes</td>\n",
       "      <td>0.969064</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.860927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.979118</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.912052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosted</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.890365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ET</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adaptive Boosting</td>\n",
       "      <td>0.964424</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.849673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.871166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RF with XGBoost</td>\n",
       "      <td>0.940449</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.654321</td>\n",
       "      <td>0.733564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.966744</td>\n",
       "      <td>0.928058</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LTSM</td>\n",
       "      <td>0.964424</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.855346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.882716</td>\n",
       "      <td>0.899371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Method  Accuracy  Precision    Recall        F1\n",
       "0   Logistic Regression  0.961330   0.850000  0.839506  0.844720\n",
       "1           Nave Bayes  0.969064   0.912162  0.833333  0.870968\n",
       "2                   SVM  0.967517   0.928571  0.802469  0.860927\n",
       "3                    RF  0.979118   0.965517  0.864198  0.912052\n",
       "4      Gradient Boosted  0.974478   0.964029  0.827160  0.890365\n",
       "5                    ET  0.975251   0.971014  0.827160  0.893333\n",
       "6     Adaptive Boosting  0.964424   0.902778  0.802469  0.849673\n",
       "7               XGBoost  0.967517   0.865854  0.876543  0.871166\n",
       "8       RF with XGBoost  0.940449   0.834646  0.654321  0.733564\n",
       "9                   CNN  0.966744   0.928058  0.796296  0.857143\n",
       "10                 LTSM  0.964424   0.871795  0.839506  0.855346\n",
       "11                 BERT  0.975251   0.916667  0.882716  0.899371"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d947c8f7",
   "metadata": {},
   "source": [
    "### Creating a Train Data Predictions DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c107f",
   "metadata": {},
   "source": [
    "This dataframe will be used to first identify false positives and then calculate the jaccard score amongst false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f3100d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yy = s_pandas['label'].iloc[train_index]\n",
    "# train_predictions = [lrr, gnr, bnr, mbr, sbr2, rfr2, gbr2, etr2, adr2, xgr2, cnr, ltr]\n",
    "# for i in train_predictions:\n",
    "#     i = i.tolist()\n",
    "# train_results = list(zip(yy,lrr, gnr, bnr, mbr, sbr2, rfr2, gbr2, etr2, adr2, xgr2, cnr, ltr))\n",
    "# df_results = pd.DataFrame(data=train_results,columns=['actual','LG','GNB','BNB','MNB','SVM','RF','GBT','ET','ADA','XGB','CNN','LTSM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "628536e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn_list = []\n",
    "# fp_list = []\n",
    "# fn_list = []\n",
    "# tp_list = []\n",
    "# for i in range(len(train_predictions)):\n",
    "#     tn, fp, fn, tp = confusion_matrix(yy, train_predictions[i], labels=[0, 1]).ravel()\n",
    "#     tn_list.append(tn)\n",
    "#     fp_list.append(fp)\n",
    "#     fn_list.append(fn)\n",
    "#     tp_list.append(tp)\n",
    "# confusion = list(zip(method_name,tn_list,fp_list,fn_list,tp_list))\n",
    "# df_confusion = pd.DataFrame(data=confusion,columns=['Method','True Positive','False Positive','False Negative','True Positive'])\n",
    "# df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be12f4",
   "metadata": {},
   "source": [
    "### Creating a Test Data Predictions DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1ffa39fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yz = s_pandas['label'].iloc[test_index]\n",
    "# test_predictions = [lrt, gnt, bnt, mbt, sbt2, rft2, gbt2, ett2, adt2, xgt2, cnt, ltt]\n",
    "# for i in test_predictions:\n",
    "#     i = i.tolist()\n",
    "# test_results = list(zip(yz,lrt, gnt, bnt, mbt, sbt2, rft2, gbt2, ett2, adt2, xgt2, cnt, ltt))\n",
    "# df_results_test = pd.DataFrame(data=test_results,columns=['actual','LG','GNB','BNB','MNB','SVM','RF','GBT','ET','ADA','XGB','CNN','LTSM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1270d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn2_list = []\n",
    "# fp2_list = []\n",
    "# fn2_list = []\n",
    "# tp2_list = []\n",
    "# for i in range(len(test_predictions)):\n",
    "#     tn, fp, fn, tp = confusion_matrix(yz, test_predictions[i], labels=[0, 1]).ravel()\n",
    "#     tn2_list.append(tn)\n",
    "#     fp2_list.append(fp)\n",
    "#     fn2_list.append(fn)\n",
    "#     tp2_list.append(tp)\n",
    "# confusion2 = list(zip(method_name,tn2_list,fp2_list,fn2_list,tp2_list))\n",
    "# df_confusion2 = pd.DataFrame(data=confusion2,columns=['Method','True Positive','False Positive','False Negative','True Positive'])\n",
    "# df_confusion2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a1e8a",
   "metadata": {},
   "source": [
    "### Creating a DataFrame of False Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "1b62e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fp_list = []\n",
    "# for i in method_name:\n",
    "#     fp_list.append(np.where((df_results['actual'] == 1) & (df_results[i] == 0),1,0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b4534bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fp = pd.DataFrame(data=fp_list)\n",
    "# df_fp = df_fp.transpose()\n",
    "# df_fp = df_fp.set_axis(method_name,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "1b26567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(method_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ef529",
   "metadata": {},
   "source": [
    "### Jaccard Score Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "79a34f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate the jaccard scores\n",
    "# method1 = []\n",
    "# method2 = []\n",
    "# jac = []\n",
    "# for x in method_name:\n",
    "#     for y in method_name:\n",
    "#         method1.append(x)\n",
    "#         method2.append(y)\n",
    "#         jac.append(jaccard_score(df_fp[x],df_fp[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "a09027e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #put into a matrix\n",
    "# d = list(zip(method1,method2,jac))\n",
    "# df_j = pd.DataFrame(data=d,columns=('method1','method2','jac'))\n",
    "# table = pd.pivot_table(df_j,values='jac',index='method1',columns='method2',aggfunc=np.sum)\n",
    "# table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2089f8",
   "metadata": {},
   "source": [
    "### Testing False Positive Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "17472b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ET_train_predictions = etm.predict(df_train).tolist()\n",
    "# ADA_train_predictions = adm.predict(df_train).tolist()\n",
    "# actual = y_train.tolist()\n",
    "# results = list(zip(actual,ET_train_predictions,ADA_train_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7b5ef848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results = pd.DataFrame(data=results,columns=['actual','ET','ADA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0f641964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results.to_excel(\"FP.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "25838076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_metrics(ET_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "7bc10b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict_metrics(ADA_train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d21f483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# tn, fp, fn, tp = confusion_matrix(actual, ADA_train_predictions, labels=[0, 1]).ravel()\n",
    "# print(tn, fp, fn, tp)  # 1 1 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9e74e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(actual, ET_train_predictions, labels=[0, 1]).ravel()\n",
    "# print(tn, fp, fn, tp)  # 1 1 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b4ae4ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "425e249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results['ADA_FP'] = np.where(df_results['ADA'] == 0 & df_results['actual'] == 1,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "95bbb848",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sum(df_results.ADA_FP.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "d9ad592b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,0,0,0,1,0,0,1,0,0]\n",
    "b = [0,1,0,0,0,0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "74e9aa02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "131da283",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [0,1,0,0,0,0,0,0,0,0]\n",
    "d = [0,0,0,0,0,0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a231b484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_score(c,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085aba74",
   "metadata": {},
   "source": [
    "### Pickle Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "35a90789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle predictions\n",
    "# with open('session.pkl', 'wb') as f:\n",
    "#     pickle.dump({\n",
    "#             'cnt' : cnt,\n",
    "#             'ltt' : ltt,\n",
    "#             'lrt' : lrt,\n",
    "#             'gnt' : gnt,\n",
    "#             'bnt' : bnt,\n",
    "#             'mbt' : mbt,\n",
    "#             'srt' : srt,\n",
    "#             'spt' : spt,\n",
    "#             'slt' : slt,\n",
    "#             'rft' : rft,\n",
    "#             'gbt' : gbt,\n",
    "#             'ett' : ett,\n",
    "#             'adt' : adt,\n",
    "#             'xgt' : xgt,\n",
    "#             'xrt' : xrt,\n",
    "#             'cnr' : cnr,\n",
    "#             'ltr' : ltr,\n",
    "#             'lrr' : lrr,\n",
    "#             'gnr' : gnr,\n",
    "#             'bnr' : bnr,\n",
    "#             'mbr' : mbr,\n",
    "#             'srr' : srr,\n",
    "#             'spr' : spr,\n",
    "#             'slr' : slr,\n",
    "#             'rfr' : rfr,\n",
    "#             'gbr' : gbr,\n",
    "#             'etr' : etr,\n",
    "#             'adr' : adr,\n",
    "#             'xgr' : xgr,\n",
    "#             'xrr' : xrr\n",
    "#                 }, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a6559",
   "metadata": {},
   "source": [
    "### Scratchwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "06b1823c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_one = [0,1,1,0,1,0,0,1,0,0]\n",
    "# pred_two = [0,0,0,0,1,0,0,1,0,0]\n",
    "# actual = [0,1,0,0,1,0,0,1,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "485f4977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.accuracy_score(actual, pred_one))\n",
    "# print(metrics.precision_score(actual, pred_one))\n",
    "# print(metrics.recall_score(actual, pred_one))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "ccf289e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.accuracy_score(actual, pred_two))\n",
    "# print(metrics.precision_score(actual, pred_two))\n",
    "# print(metrics.recall_score(actual, pred_two))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "e4b4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(metrics.accuracy_score(y_test, ett2))\n",
    "# print(metrics.precision_score(y_test, ett2))\n",
    "# print(metrics.recall_score(y_test, ett2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "aafafe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tn, fp, fn, tp = confusion_matrix(y_test, ett2, labels=[0, 1]).ravel()\n",
    "# print(tn)\n",
    "# print(fp)\n",
    "# print(fn)\n",
    "# print(tp)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500dc708",
   "metadata": {},
   "source": [
    "### Optimized Voting Assemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3de2075c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_grid = []\n",
    "precision_grid = []\n",
    "opt_grid_name = []\n",
    "methods_grid_name = ['Logistic Regression','Nave Bayes','SVM','RF','Gradient Boosted','ET','Adaptive Boosting','XGBoost','RF with XGBoost','CNN','LTSM','BERT']\n",
    "methods_grid = [lrv,mbv,sbv2,rfv2,gbv2,etv2,adv2,xgv2,xrv,cnv,ltv,bev]\n",
    "for x in range(len(methods_grid)):\n",
    "    for y in range(len(methods_grid)):\n",
    "        for z in range(len(methods_grid)):\n",
    "            a = methods_grid_name[x]\n",
    "            b = methods_grid_name[y]\n",
    "            c = methods_grid_name[z]\n",
    "            if ((a == b) or (a == c) or (b == c)):\n",
    "                name = 'xxx'\n",
    "            else: \n",
    "                name = a + ',' + b +',' + c\n",
    "            opt_grid_name.append(name)\n",
    "            f1_grid.append(metrics.f1_score(y_val,vote_prediction(methods_grid[x],methods_grid[y],methods_grid[z])))\n",
    "            precision_grid.append(metrics.precision_score(y_val,vote_prediction(methods_grid[x],methods_grid[y],methods_grid[z])))\n",
    "df_vote_grid = pd.DataFrame(\n",
    "    {'Method': opt_grid_name,\n",
    "     'F1': f1_grid,\n",
    "     'Precision': precision_grid\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6afb7fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_vote_grid[df_vote_grid['Method'] != 'xxx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "28f18307",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.sort_values(by=['F1','Precision'],ascending=False).to_excel('f1_grid.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ac92a3",
   "metadata": {},
   "source": [
    "### Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "77b66e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.961330</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.844720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nave Bayes</td>\n",
       "      <td>0.969064</td>\n",
       "      <td>0.912162</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.870968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.860927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RF</td>\n",
       "      <td>0.979118</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.864198</td>\n",
       "      <td>0.912052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gradient Boosted</td>\n",
       "      <td>0.974478</td>\n",
       "      <td>0.964029</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.890365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ET</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>0.971014</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.893333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Adaptive Boosting</td>\n",
       "      <td>0.964424</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.802469</td>\n",
       "      <td>0.849673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.967517</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.876543</td>\n",
       "      <td>0.871166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>RF with XGBoost</td>\n",
       "      <td>0.940449</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.654321</td>\n",
       "      <td>0.733564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CNN</td>\n",
       "      <td>0.966744</td>\n",
       "      <td>0.928058</td>\n",
       "      <td>0.796296</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LTSM</td>\n",
       "      <td>0.964424</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.855346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.975251</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.882716</td>\n",
       "      <td>0.899371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sighn Voting Ensemble</td>\n",
       "      <td>0.977572</td>\n",
       "      <td>0.965035</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.904918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Alternative Voting Ensemble</td>\n",
       "      <td>0.978345</td>\n",
       "      <td>0.971831</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.907895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Method  Accuracy  Precision    Recall        F1\n",
       "0           Logistic Regression  0.961330   0.850000  0.839506  0.844720\n",
       "1                   Nave Bayes  0.969064   0.912162  0.833333  0.870968\n",
       "2                           SVM  0.967517   0.928571  0.802469  0.860927\n",
       "3                            RF  0.979118   0.965517  0.864198  0.912052\n",
       "4              Gradient Boosted  0.974478   0.964029  0.827160  0.890365\n",
       "5                            ET  0.975251   0.971014  0.827160  0.893333\n",
       "6             Adaptive Boosting  0.964424   0.902778  0.802469  0.849673\n",
       "7                       XGBoost  0.967517   0.865854  0.876543  0.871166\n",
       "8               RF with XGBoost  0.940449   0.834646  0.654321  0.733564\n",
       "9                           CNN  0.966744   0.928058  0.796296  0.857143\n",
       "10                         LTSM  0.964424   0.871795  0.839506  0.855346\n",
       "11                         BERT  0.975251   0.916667  0.882716  0.899371\n",
       "12        Sighn Voting Ensemble  0.977572   0.965035  0.851852  0.904918\n",
       "13  Alternative Voting Ensemble  0.978345   0.971831  0.851852  0.907895"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singh_voting = vote_prediction(cnt,gbt2,rft2)\n",
    "cl_voting = vote_prediction(cnt,gbt2,bet)\n",
    "\n",
    "methods = [lrt,mbt,sbt2,rft2,gbt2,ett2,adt2,xgt2,xrt,cnt,ltt,bet, singh_voting,cl_voting]\n",
    "method_name =  ['Logistic Regression','Nave Bayes','SVM','RF','Gradient Boosted','ET','Adaptive Boosting','XGBoost','RF with XGBoost','CNN','LTSM','BERT',\"Sighn Voting Ensemble\",\"Alternative Voting Ensemble\"]\n",
    "accuracy3 = []\n",
    "precision3 = []\n",
    "recall3 = []\n",
    "f13 = []\n",
    "for i in methods:\n",
    "    accuracy3.append(class_accuracy(i))\n",
    "    precision3.append(class_precision(i))\n",
    "    recall3.append(class_recall(i))\n",
    "    f13.append(class_f1(i))\n",
    "results3 = pd.DataFrame(\n",
    "    {'Method': method_name,\n",
    "     'Accuracy': accuracy3,\n",
    "     'Precision': precision3,\n",
    "     'Recall':recall3,\n",
    "     'F1':f13\n",
    "    })\n",
    "results3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
